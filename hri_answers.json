{
    "1": [
        "Robotics is concerned with solving a specific class of problems in the continouus physical space. The problems usually deal with how we most efficiently manipulate or interact with the world (i.e. \u201cgo from A to B in the least amount of time\u201d or \u201ccompute a secure grasp for this object with this gripper\u201d).\n\nHRI is concerned with how humans interact with the robot. Whenever there is a human on either the commanding end or in the recieving end, it is a HRI problem. The problems are related to collaboration, communication, user experience, how we most effeciently leverage the robots, developing good requirement specifications, social cues and culture, etc.",
        "It can be used to classify user intention in a less invasive way than EEG.\n\nIt can be used in collaborative robots in conjunction with the normal control method - the user\u2019s attention (captured via eyetracking) can be passed to a ML model in order to infer the user\u2019s intention. The intention can then be fused with the ordinary command, giving a richer command in the end.\n\nIt also has uses in non-collaborative settings. Consider a self-driving car - has the user registered a swerving pedestrian or cyclist?",
        "We have some wearable sensors available to us, such as:\n\n- RGB-D camera (used for estimating hand or body poses)\n- IMU (shake to undo, report an error in the app)\n- EMG (direct control/pattern recognition control)\n- EEG (brain waves, either on scalp, in ear, or underneath skull)\n\nThese can be classified and used to control the robots.",
        "Robots are embodied and active in the real world - they inhabit some physical space, and their actions impact the state of the universe. Therefore, the robot needs to be generalized to a broad spectrum of social backgrounds, ages and even cultures.\n\nDepending on these factors, the user might have different instincts and even trust of technology. An old person will have less trust in the robot, whereas a younger person will put more trust in the robot.\n\nAn example of \u201cuntrained operators\u201d could be a restaurant guests. They are not aware of the inner workings and training datasets for the robot, so the robot needs to generalized. Humans and the universe is inherently chaotic, so the robot is at a constant risk of being operated \u201coutside the lines\u201d.\n\nIf the robot is not sure of what to do (i.e. it does not meet a certain confidence threshold in the inner model), it should have graceful fallback mechanism. A big issue is safety - from a utilitaristic viewpoint, it needs to be better than before.",
        "In 4.0, there is lots of focus on smart automation, IoT, cloud, cyber security etc. Humans mostly oversee the robots and dont actively work besides them. Systems are autonomous and datadriven.\n\n5.0 has a more human-centric approach. Lots of things in 5.0 are about sustainability, but lots of things is also about empowering the humans to work alongside the robots instead of \u201cabove them\u201d. \n\nThis is accomplished by lots of things, such as:\n\n- NLU, multilingual speech, intention recohnition\n- Cobots (how do humans safely and efficiently work with robots)\n- Enhanching cognitive and physical capabilities of humans (i.e. decision support systems with ML or exoskeletons)\n- VR and AR\n- Tracking, datadriven systems for monitoring staff health"
    ],
    "2": [
        "It can be seen as a way of fusing sensor data. We observe something with our eyes, and we have an expectation of the bodily sensation of this \u201csomething\u201d - a brush will feel soft, a rock will be hard, a steaming cup of coffee will be warm, etc.\n\nBut we also have an actual sensory outcome. How do the sensory organ in our body actually sense this? Is the cop actually warm?\n\nIf the two senses match up, there is congruence, and the agency, ownership and embodiment increases. If they don\u2019t there is incongruence, and you lose the aforementioned properties.\n\nExplain with \u201cThe Rubber Hand\u201d",
        "This is the actual sensor fusion, or can be seen as the posterior belief of the sensory prediction. You combine whatever you have available, and then come up with a final most likely prediction. The optimality comes from that we want to minimize variance.\n\nVery context dependent - this influences how the beliefs are combined.\n\nExample of adding yet another modality: hearing/cognition. You are told that it is actually a rock that is touching you even though you see and feel something very soft. You can physically see that it is not a rock, so it has a very low probability.",
        "- Ownership is about how much it feels like the hand is part of your own body, the feeling of looking at the hand, and overall having a sense that the fake hand/foreign object is assimilating the characteristics of the thing you are trying to replace.\n- Location is related to a causality between where your hand is and the rubber hand is - are they physically in the same place?\n- Agency is about how you might be able to move the rubber hand and feel it.\n\nAnother distinction is asynchronous/synchronous - are the sensations caused at same time? When done async., all three properties are not achieved (i.e. subject disagreed). However, ownership and location was positive (i.e. subject agreed) when the sensation was done sync.. This is likely because the hand is not actuated. It is completely \u201cdead\u201d.",
        "First, the higher degree of embodiment a prosthesis has, the easier it is to use. Ease of usage and having a non-cumbersome prosthesis is very important, so it is fair to draw the conclusion of embodiment = higher QoL for amputees. Since they will actually like using it and it will help them in everyday tasks.\n\nPhantom limb can at best cause weird sensations (elbows bending wrong direction, fingers in palm etc), but at worst also cause very bad pain. With embodied prostheses, you can relieve some of this pain.",
        "For example, there is noninvasive methods where remaining senses (i.e. upper arm, tongue) are stimulated according to some sensor input (camera or mechanoreceptors). But there are also invasive methods where you actually stimulate the nerves themselves inside the actual muscles.\n\nSubstitution is the act of replacing a lost sense, e.g. losing eyesight. Supplementation is adding something entirely new, like a specific vibration pulse rhythm when you have reached maximum torque in your prosthesis.",
        "Most normal approach is using questionnaires, but some it is not ideal. It is a very qualitative measures where you will get responses such as \u201cI felt that the rubber hand was like my own hand\u201d.\n\nAnother novel method is eye-tracking. With this, you can \u201cpeep\u201d into the subconsciousness. Does user explicitly need to rebase their prosthesis all the time? Or are they able to reliably just let go of the objects?"
    ],
    "3": [
        "BCI is about creating some kind of link between the brain of the user and the prosthesis/device. For example, you might wear an EEG cap (or an EEG sensor in the ear), which measures the potential of the brain signals, which can then be preprocessed, filtered, and classified.\n\nIt can also go the other direction. You can provide feedback from the prosthesis into the brain, i.e. from force sensors on the prosthesis. This improves embodiment and ultimately allows fine-motor control since you can have a better insight into the type of pressure you are applying.\n\nBCI also exists in more invasive forms, where you surgically implant a sensor directly on the cortex, under the skull (neuralink).",
        "From ShitGPT:\n\n*Nerves and muscles produce electrical signals because it\u2019s how the body communicates and controls movement. Electrical impulses let the brain send fast, precise instructions to muscles and organs, and help muscles respond instantly. It\u2019s the body\u2019s natural way of wiring thought to action, enabling everything from blinking to walking.*\n\nThere are 2 types of nerve fibers: afferent (sensory neurons) and efferent (motor neurons). Afferent neurons carry signals from body to brain, where efferent neurons carry signal from brain to body. Afferent neurons outnumber efferent neurons by a factor of 9 to 1, so it is very important for the body to have sensory feedback.",
        "|  | Pros | Cons |\n| --- | --- | --- |\n| Brain | - Record signals directly from their place of origin\n- Could theoretically 100% reenable natural senses | Weak signal if done non-invasively, and hard to gather enough focus |\n| Spinal cord | - Less invasive than brain\n- Can capture motor commands after they have left brain | - Still invasive\n- Hard to target specific muscles |\n| Peripheral nerves | - Easy to target specific muscles\n- Can use afferent fibers directly | - Unstable\n- Difficult to create stable interface |\n| Muscles | - Non-invasive\n- Ease of use | - Sensitive to sensor placement, so might need frequent recalibration\n- Can be tiring |\n\nFor some type of prostheses, you might have many DOFs you need to control, and to create a naturally feeling control scheme, you may need to be able to handle them all directly. Otherwise you will need to resort to direct control.\n\nIt might also be that the spinal/brain placements are overkill if user wishes a simple upper-limp prosthesis. If the user has a specific wish for a more embodied prosthesis with feedback, and is willing to accept the invasiveness, the intrabodily implants might be the best solution.\n\nA whole other thing is the amount of functionality the user has lost. If the user is fully parallyzed, muscle based sensors will not work.",
        "Advantages are that it captures a cleaner signal that is less sensitive to crosstalk. Electrode placement becomes less of an issue (as is often the case with EMG), since the specific targeted muscle are directly targeted by the implants.\n\nDisadvantages is the invasive surgery. Many users (probably predominantly in the elderly section of society) will have a distrust for electronic aids, so a simpler device will feel safer for them to use. In many cases, surface EMG will be good enough for the task at hand.",
        "Firstly, user needs to be explicitly aware of the implications of wearing something with metal inside their body. If the device contains a specific type of metal, it could be very dangerous in MR scanning.\n\nAs clinicians, you need to keep everything sterile so no infections occurs because of the foreign object. The system should also be implanted in such a way that it can resist the bodily forces (i.e. an EMG electrode array should flex enough to not break when user elongates their muscle).\n\nEverything needs to be insulated properly so you don\u2019t subject the user to electric shock. Could also impose some issues with heating."
    ],
    "4": [
        "The pros is that you can increase ownership. If your prosthesis looks and behaves like a human arm, the embodiment increases. If robot is able to conform to proxemis, use real human gestures etc, it will feel nicer to interact with. Consequently, trust also goes up.\n\nThere is the notion of *design affordance*. When a product or a system is designed in a specific way, its design may impose some expectations from the users. If an end-user encounters a big red button, it will usually be associated with a stopping mechanism. And then, if the user sees a humanoid/anthropomorphic robot, the user will expect it to behave like a robot.\n\nSo this might be a con - if you design it as a human, you need to follow through. You essentially make it hard for yourself and risk going into \u201cuncanny valley\u201d. There is also another perspective - certain groups of society will not be able to distinguish between real and robot (i.e. snapchat AI for lonely teens).",
        "All three are necessary to create good HRI (and products in general), but they have conflicting interests:\n\n- Designer\u2019s knowledge is based on intution (implicit). Engineer and scientist is based on facts and science (explicit)\n- Scientist wants to understand the world. Engineer and designer wants to transform the world\n- Engineer is interest in technology. Designer and scientist is interested in humans and users.\n\nAs with all projects, this conflict of interests and different knowledge bases is never beneficial. Therefore, we might need to wear multiple hats, which causes gaps in the product.",
        "Take HERA (now named ARUS) as example. The actual operators and patients of the robot are not technical in any sense. They are simply medical personnel. Compare this with a factory floor. Here, it is not totally unthinkable to have technical employees onsite. This imposes a need for userfriendlyness.\n\nIn the case of prostheses, the same applies. If the control scheme (i.e. your interaction with the robot) is not very good and not very embodied, the user will simply not use your project. It is such an integral part of the user as a human, that it is very important and very hard to get right.\n\nLastly, the consequences of medical robots are also high. If something goes wrong with a pacemaker, it is very dangerous. If you experience an error in a robotic arm at a car assembly factory, that just means Toyota loses 50.000 dollars for that day.",
        "It is difficult for the operators to get a true feedback loop. They can visually see the robotic grippers and end effectors, but they dont have any haptics or other sensory input. Consequently, they express a wish for sensory substitution. \n\nA big part of surgery is the experience the surgeons have. Whenever they perform any kind of task (i.e. surturing a wound) a lot of that knowhow comes from how much pressure they are applying with the needle. So if HRI of DaVinci is bad, it might actually make the robot worse than the current status-quo, which is a no-no.",
        "For the system, there are factors such as reliability or predictability. However, there is also the factor of the user. Do they experience a sense of embodiment? Do they have a sense of agency and control to the point where they feel confident in operating the robot?\n\nSome actual points:\n\n- Visual appeal: If robot looks like a human and can act like a human, it may increase trust. Or differently, if the robot is cute and non-Terminator like\n- Feedback loop: Having a sense of the robot\u2019s actions can improve agency. Robot is actually doing what you want it to do.\n- Safety: If robot has no potential risk attached to it, people trust it more\n\nYou need to hit a middle ground between the actual trustworthiness of the robot and the user\u2019s perceived trustworthiness of the robot. If the users trust the robot too much, they will overtrust it and potentially cause harm (i.e. it has too much autonomy like a selfdriving car). If the users don\u2019t trust it enough, they will simply not use your product.",
        "- Patients\n    - They expect to feel safe\n    - They want the interaction to improve at least prevention/diagnosis/treatment/recovery, which imposes different things depending on the actual robot\n        - They are motivated by getting a solution that in some way improve the existing solution (i.e. a human doctor)\n    - Speed\n    - Automation\n    - No increased risks\n- Clinician\n    - They expect system to be ergonomically compliant.\n    - It should have a good accuracy, be able to provide feedback. Must be integratable in the existing workflow.\n    - In some cases, they will also expect the robot to be small in form-factor so it can fit in operating rooms.\n    - They are motivated by getting an improved system for decision making.\n    - Data integration so it can somehow provide tracking (in line with Industry 5.0)",
        "- Ergonomics: This will potentially be an integral part of clinician\u2019s workday, so the interaction needs to be performed in a way so it will not cause wear-and-tear injuries\n- Workspace: The environment potentially has steep requirements in terms of sterility and infection risks, so that imposes very concrete requirements of materials etc. Also needs to be non-interfering with the pathways and people in the operating room\n- Footprint: Hospitals are very constrained in their physical limits. They will need to work with the space they have, so renovating operating rooms/other facilities will most likely not be an option, especially if host hospital is in public sector of the country",
        "Utilitarianism is the idea of providing the best possible solution for the most amount of people, even if some people \u201cget left behind\u201d by the thing you are changing/trying to implement.\n\nThe classic example is the runaway train that will by default hit the kindergarten (i.e. lots of small children that have a long life and taxpayer potential ahead of them). But you can then flip a switch so you actively choose to hit a grandmom instead.\n\nQuality Adjusted Life Year (QALY) is a scaled year in your life based on how healthy you are in that year:\n\n- Living 1 year in perfect health = 1 QALY\n- Living 1 year with moderate disability or pain = maybe 0.6 QALY\n- Living 1 year bedridden and in severe pain = maybe 0.2 QALY\n\nIt\u2019s a very utilitarianistic way of estimating the worth of human lives. If you have a treatment available, and it costs more than 1-3 GDP pr. capita of your country, you will not implement it. If it costs less than that, it will save more money than it costs, and it is worth doing.\n\nSo the utopian WHO recommendation puts a high price on human beings - we are worth a lot of money. However, in practice, it is often valued at less than 1GDP pr. capita - in other words, more treatment options are being discarded.",
        "- Transparency \u2192 if the user doesn\u2019t have the sensation of a black box, they will be less skeptical\n- Credibility \u2192 if the actual predictions are acceptable, it is beneficial\n- Auditability \u2192 accountability can be easily measured and you have a place to put the blame if something goes wrong\n- Reliability  \u2192 The system performs as intended, and you are given a guarantee of service\n- Recoverability \u2192 manual control can be assumed if required, so the user is not \u201cputting all their eggs in the one basket\u201d",
        "If the system does a wrong prediction that has severe consequences (injury or loss of life), who is to blame? Is it the system itself, is it the clinical personel or is it the engeering team? There will not always be a clear answer, and it is easy to imagine lots of finger-pointing.\n\nThe AI system must adhere to these ethical principles\n\n- Beneficence and non-malevolence \u2192 system must respect patient\u2019s best interest and respect their choice\n- Respect of autonomy \u2192 A patient\u2019s choices are all-powerful\n- Trust \u2192 We put trust in the system, so it must protect data, eliminate bias and work within the legislation\n- Human autonomy and privacy \u2192 Consenting to the use of information required for AI?\n\nAlso some very concrete challenges like: in case of failures, what actually happens in terms of legal repercussions? Healthcare authorities are vastly different across the world with very different interpretations of responsibility.",
        "If you are fully comatose or is completely incapable of giving consent (Metallica - One style), your brainwaves might be the very last modality of communication you have left. Therefore, it is an opportunity to give otherwise unreachable patients a voice. Also very relevant from an ethical standpoint, there was a case in Denmark recently \u2192 how do we treat patients in coma?\n\nHowever, BCI via EEG has some inherent noise, and as with all ML techniques using softmax or any form of learned procedure, you will never get a completely clear answer. You will never know if the patient\u2019s intent is what the algorithm say it is.",
        "The context is that you are doing some kind of test that involves test subjects. This specific test involves children with autism diagnoses, so it is a very at-risk group. Therefore, there are some ethical implications about the test\n\nThe participant information is simply a document explaining what kind of test the subject will be performing, informing them of consent withdrawal, what the test seeks to achieve etc.\n\nThe ethical research protocol is then an application to some kind of ethical authority in the place of experiment. It needs to describe various factors of the test, such as risk assessment, insurance policies, the clinically responsible, experiment background, statistics handling, data handling etc.\n\nYou send all this informatio with appendices etc, and then you get a reply back after 60 days."
    ],
    "5": [
        "It is a way of interpreting the world and how its inhabitants behave. Agents are integrated systems in their environment:\n\n- They perceive the environment and act in it\n- They use sensors and effectors to affect the environement\n- They can either be deliberative (if they have cognitive abilities) or reactive (if they act by reflex)\n\nThe takeaway is that the agents are active parts of the universe: Based on the context they find themselves in, they will have a given way of acting, either deliberately or by reflex. This allows them to adapt to specific context which might call for different cues.\n\nConsider Kendon\u2019s F-formation system for spatial group formation: An agent will perceive this pattern and consequently plan its actions based on this. \n\nAnother takeaway is that agents are considered on equal terms (in this sense) as the other agents of the world. They are upheld to the same standards as the humans, meaning that they must follow a certain etiquette of behaving.",
        "Gestures have a communicative function, and they convey specific meanings depending on context and culture. A classic example is how people will sometimes tell you to NEVER do a thumbs-up in [insert-random-country] because it is equivalent to the western middle finger. Gestures might also express a certain mood or emotional state of the user.\n\nThere are many ways to categorize gestures - some involve speech/non-speech linked, some involve where the gesture is performed relative to the body. \n\nThere is also a \u201cmeta\u201d dimension to gestures by considering if they should even be included. Gestures are inherently very human, and because of design affordance, you might not actually wish for your robot to use gestures.",
        "These parameters describe the way a given gesture is performed by the agent. Very \u201cloud\u201d gestures might contain some kind of data that convey you are from a more extroverted, southern country. They can even convey some kind of personality trait about yourself.\n\nThere are these concrete examples of expressivity parameters:\n\n- Spatial volume \u2192 describes amplitude of the movement of arms, wrists, head, and eyebrows\n- Speed \u2192 describes how fast a movement is performed\n- Energy \u2192 describes a gesture or expression overshooting, i.e. gestures reaches a point that is beyond the final one of the movement\n- Fluidity \u2192 describes how continuous the movement is\n- Repetition \u2192 describes how often a movement is executed",
        "There is a very concrete set of circular ranges around your origin that correspond to different social contexts - or put differently, how comfortable you are with agents moving within these ranges. \n\nThe ranges are:\n\n- 7,6m - 3,6m \u2192 public space\n- 3,6m - 1,2m \u2192 social space\n- 1,2m - 0,45m \u2192 personal space\n- 0,45m - 0m \u2192 intimate space\n\nThis is a very static and potentially stereotypical way of distinguishing between boundaries. But nonetheless very culturally dependent and as such reproducible.\n\nIf you have a museum robot, you want to improve your users\u2019 experience so they will come back to your museum and spend money. Therefore, you want their experience to be as good as possible. If the robot can establish a good amount of trust with the robot, you achieve your goal.\n\nIf robot agent trespasses the acceptable limits as per proxemics, they break the social contract and the users will not be as satisfied.",
        "Affective robots could be found in elderly care.\n\n- Perception and analysis of everyday emotional states \u2192 they need to capture users\u2019 moods. The actual perception is done with cameras or audiosensors, but lots of opinionated factors are at play in the classification. Is the user performing non-verbal gestures? What are the expressivity parameters like? What emotion is present on the face? What culture are you inhabiting? etc.\n- Generation of appropriate emotional output \u2192 based on the context and the societal group of the user, different emotional outputs are deemed polite/relevant. This is challenged by the fact robots have limited abilites in using facial expressions (human faces are very very hard to get right, and they often come out as creepy). Therefore, this challenge is perhaps best solved with ABM (affective body movements)\n- Computational models of affective interactions and the appropriateness of employing them \u2192 OCC is a cognitive model that tries to differentiate between different emotions based on the subjective interpretation and personal significance of the event. Based on this, an appropriate gesture can be employed to the user. The computational part is how it tries to balance all the parameters currently in the mix - both the user\u2019s emotional state, but also culture, proxemics, affective interactions as per OCC, etc.",
        "They live inside a very nicely numerical plane. Whenever we try to classify a human being\u2019s face with some kind of CNN/ANN/feature-based approach, it gives us a more insightful answer compared to simply giving a discrete answer.\n\nOne example of a dimensional model is Wundt all the way back from 1896: In this model, there are three dimensions:\n\n- Pleasure/displeasure\n- Arousal/calming\n- Tension/relaxing"
    ],
    "6": [
        "It is based on ISO 9241 which has the longer name: \u201cErgonomics of human-system interaction and related, human-centered design processes\u201d. It defines usability as: \u201cThe extent to which a product can be used by specified users to achieve specified goals in a specified context of use with\u201d [and then the three elements:]\n\n- Effectiveness \u2192 how accurate and complete can the users achieve goals\n- Efficiency \u2192 how resource-expensive is the task, when measuring time, effort, cost and material\n- Satisfaction \u2192 how does system meet the user\u2019s needs and expectations based on physical, cognitive and emotional response from the user",
        "- Working in teams that include skilled usability specialists, interface designers, and technical\ndesigners\n- Engineering it into a product through an iterative design and development process\n- Involving the users throughout the design process\n- Allowing usability and users\u2019 needs to drive design decisions\n- Setting quantitative usability goals early in the process\n- Testing products for usability, but also integrating usability testing with other methods for ensuring usability\n\nSo I think the most three intuitive methods are:\n\n- Incorporate actual UX specialists and technical designers\n- Use an iterative design process with rapid prototyping that incorporates user feedback\n- Find a way of quantizing usability and set concrete goals",
        "When you conduct your tests, you may do it in three ways:\n\n- Observing a single test subject and doing an interview at the very end\n- Actively intervening in the test and contiously probing test subject\n- Co-discovery \u2192 you have multiple test subjects at once and try to facilitate a conversation between them. If you talk with someone, it may feel more natural and you may uncover hidden details of your experience with the test",
        "| Aspect | Usability test | Functionality test |\n| --- | --- | --- |\n| Primary goal | Uncovering problems with the system | Demonstrating the existence of some specific phenomenum or evaluating functionality |\n| Participants | Real users of the system | The engineers/designers themselves may test the system |\n| Experimental procedure | Fewer variables, more emphasis on observing user behavior | More variables, evaluate system under different premises |",
        "Firstly, it is a new market, so the landscape for standards and evaluation methods is not very developed. Furthermore, lots of the operators and users will be elderly people or clinicians that lack technical knowhow. Therefore, usability is extra important as this will be the barrier of entry of using the objects.\n\nOn a more concrete level, it is actually a legal requirement for medical devices:\n\n- IEC specifies that manufacturers should implement a usability engineering process\n- FDA requires that a proof of usability related to the development of the medical device",
        "Jakob Nielsen came up with it in 1989 when status quo was very rigid and expensive usability studies. Because it was so intense, most projects simply didn\u2019t do any usability work, hence software was pretty shit.\n\nDiscount usability testing relies on ground thesis of \u201csomething is better than nothing\u201d. It has three main ideas:\n\n- Simple user testing (5 users)\n- Simple prototypes = early testing\n- Heuristic evalutation (essentially simple \u201crule of thumb\u201d checks)"
    ],
    "7": [
        "Each subsequent addition to the language field is a subset of the previous. They are:\n\n- Automatic speech recognition (ASR) \u2192 this is not \u201csmart\u201d and relies on simple patterns to infer patterns in the speech. A big thing is that the term does not contain \u201clanguage\u201d\n- Natural language processing (NLP) \u2192 this is a step up in complexity. Here you are analyzing patterns in the language to infer some kind of category and syntax labeling\n- Natural language understanding (NLU) \u2192 this is the final boss. This is where semantics are parsed, dialogue summarization etc\n\nLLM greatly enhances NLU and task execution for robots. It is extremely generalized and can handle many different types of situtions and contexts. It provides semantic awareness and can infer lots of emotional states in the user.\n\nThere has been lots of research and industry work in applying these naturally to robots. One example is Google\u2019s RT-2, a big foundation model, which relies on LLMs. It tries to come up with a task plan, where the semantics of a given video are extracted.\n\nIn a HRI context, they are useful, because they allow robots to naturally communicate with humans. As per design affordance, this is a big thing, and they will help the integration of robots into society.",
        "- Few shot prompting \u2192 you provide a context to the answer you are expecting to recieve. This contextual demonstration conditions the model and allows it to better tune in to the context you are expecting. However, there is not an explicit reasoning step, for which you might use\u2026\n- Chain of thought prompting \u2192 this enables complex reasoning by defining an intermediate reasoning step. So when the model is decoding the feature vector, it is able to incorporate the information from the reasoning step for the final answer. You are essentially explitly instructing the model in how to perform the reasoning.\n- Meta prompting \u2192 this is a very structured approach in which you provide a specific template that the reply must follow. Since you are providing this explicit format/pattern, it is usable across many domains of problems. By doing this, you are 1) enabling fair comparison across different prompts, 2) reducing token count and 3) minimize the influence of specific examples as they are all viewed equally",
        "Transformer models is a way of interpreting and handling data that has some hidden temporal insight - such as natural language. A word I say in the beginning of the sentence might provide important context for later parts of the sentence.\n\nTraditionally, people have tried to capture this sequential info using models such as LSTM. However, this is difficult to scale, since you need to process the input strictly in order. With Transformers, you can process each token\u2019s relevance to all other tokens in parallel, allowing for these very large token windows we see.\n\nThe attention mechanism relies on computing three types of values for each token in the input: \n\n- Query \u2192 this is like computing some kind of lookup value. For token XX, what is relevant to this token?\n- Key \u2192 this is like the index. When others are considering me, what part of me will be relevant to them?\n- Value \u2192 This is the actual data I store\n\nThese values are projections and are simply computed based on learned weights. And the genius thing is that you can compute these for the entire input in parallel \u2192 massive performance gains in HPC contexts.\n\nThe multihead approach allows you to compute a set of Q, K, V-values for each token. The intuition is that you want to capture different kinds of information from the input space. In LLMs, this might mean that one of the heads capture semantic relationships (i JUMP in the POOL), whereas one might focus on inherent grammar rules to that lanuage (in Spanish, feminime nouns are often suffixed with -a), etc.\n\nLastly, it is an encoder-decoder model. At first, it encodes the input sequence into the high dimensional embedding space, capturing lots of the fine details about temporal relationships. Then, the decoder uses autoregressive attention on the feature vector. It continously only looks at the previous tokens to generate subsequent token. It also uses cross-attention on the input sequence \u2192 what parts of feature vector is most relevant to the currently generated tokens? It does this until an <EOS> token is reached.",
        "SayCan is a way of using LLM to perform task grounding. Task grounding is a way of constraining the reply space to only contain useful actions. It also does world grounding, which is a way of quantizing what actions are actually possible.\n\nThe \u201cSay\u201d is when the LLM is scoring the skill relevance (task grounding). It bridges the abstract language of the user to the discrete, concrete possible skills that are available to the robot. This weighs the skill\u2019s likelihood of making progress towards the high-level instruction.\n\nThe \u201cCan\u201d is the affordance function of each skill (world grounding). Put in other words, how likely is it for that skill to be succesfully completed when considering current state of the world and the robot. These value functions are typically learned though trial-and-error RL.\n\nSayCan combines probabilities from the LLM (i.e. usefulness of skill) with probabilites from the value function (i.e. successful execution of skill). So summarized, it is:\n\n![image.png](image%201.png)\n\nThe overall gimmick is \u201cDo As I Can, Not As I Say\u201d - because the robot will not natively be able to execute the high-level language instructions.",
        "Whenever skill A completed, the outcome of that skill is fed back into the subsequent task grounding process for what task B turns out to be. This updates the system\u2019s context and embeds it into next task and world grounding.\n\nIn other words, the \u201cSay\u201d and \u201cCan\u201d keeps running until \u201cTerminate\u201d token has been reached. However, the state of the world changes, and this is what actually steps the robot through the high level task.",
        "These value functions have the role of a measure of feasibility. If a given skill proposed by the \u201cSay\u201d gets a low score by the value function, it is considered less likely to be executed succesfully by the robot (even if it was semantically relevant). This could perhaps be because the pick-and-place object is not possible because it is not present in the environment.\n\nIf a skill is completed, robot recieves reward of 1.0 (and 0.0 if not). With the value functions, you get a guarantee that the high-level task plan is actually practically achievable by the robot.",
        "- Plan success rate \u2192 this measures whether or not the LLM\u2019s estimation of most relevant skills are actually correct when considered against the instruction\n- Execution success rate \u2192 this measures whether or not the full SayCan system actually performs the instructions as intended\n\nThis is like a validation step. Does SayCan itself propose meaningful and correct task plans? Are they achievable? The rates are determined by humans observing the robot in test scenarios."
    ]
}